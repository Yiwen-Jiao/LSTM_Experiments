{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datenaufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [1, 0, 0],\n",
      "       [0, 1, 0],\n",
      "       [0, 1, 0],\n",
      "       [0, 0, 0]])\n",
      "'+I+--0'\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "\n",
    "import trainer\n",
    "reload(trainer)\n",
    "\n",
    "sequence_length = 6\n",
    "\n",
    "reference_input_data, reference_output_data = trainer.getSequences(sequence_length)\n",
    "\n",
    "# trainer.getSequences(sequence_length) generates all possible combinations of\n",
    "# the characters '+-0I', so for a sequence length of 6 characters there are a\n",
    "# a total of 4^6 = 4096 possible combinations. Some Examples:\n",
    "# '+-+-+-' = 0\n",
    "# '------' = -6\n",
    "# '0++000' = 2\n",
    "# 'I++000' = -2\n",
    "#\n",
    "# Those sequences are encoded: Every character is representated by a vector, so the actual\n",
    "# return value from trainer.getSequences looks like this:\n",
    "pprint(reference_input_data[0])\n",
    "\n",
    "# There is a helper to decode that again:\n",
    "pprint(trainer.decodeSequence(reference_input_data[0]))\n",
    "\n",
    "# The solution for that sequence is:\n",
    "pprint(reference_output_data[0])\n",
    "\n",
    "instruction_count = np.array(reference_input_data).shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll train using 1024/4096 Examples\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = len(reference_input_data) / 4 # we use 1/4 of the data for the training\n",
    "\n",
    "test_input = reference_input_data[NUM_EXAMPLES:]\n",
    "test_output = reference_output_data[NUM_EXAMPLES:] # everything beyond NUM_EXAMPLES\n",
    "\n",
    "train_input = reference_input_data[:NUM_EXAMPLES]\n",
    "train_output = reference_output_data[:NUM_EXAMPLES]\n",
    "\n",
    "print(\"We'll train using \" + str(NUM_EXAMPLES) + \"/\" + str(len(reference_input_data)) + \" Examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, sequence_length, instruction_count], name='data')\n",
    "target = tf.transpose(tf.placeholder(tf.float32, [None], name='target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Die LSTM Schicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SETTINGS = {\n",
    "    'num_cells': 24,\n",
    "    'feature_size': 3\n",
    "}\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(LSTM_SETTINGS['num_cells'], state_is_tuple=True)\n",
    "\n",
    "lstm_predictions, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n",
    "lstm_predictions = tf.transpose(lstm_predictions, [1, 0, 2])\n",
    "\n",
    "lstm_prediction = tf.gather(lstm_predictions, int(lstm_predictions.get_shape()[0]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([LSTM_SETTINGS['num_cells'], 1]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "\n",
    "prediction = tf.matmul(lstm_prediction, weight) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost & Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_square_error_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('mean_square_error'):\n",
    "    mean_square_error = tf.reduce_sum(tf.square(tf.subtract(target, tf.unstack(prediction, axis = 1))))\n",
    "tf.summary.scalar('mean_square_error', mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cs/anaconda/envs/TestEnvironment/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'error_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('error'):\n",
    "    with tf.name_scope('mistakes'):\n",
    "        mistakes = tf.not_equal(target, tf.round(tf.unstack(prediction, axis = 1)))\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "tf.summary.scalar('error', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "date = str(datetime.datetime.now())\n",
    "train_writer = tf.summary.FileWriter('logs/selfmade_lstm/' + date + '/train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('logs/selfmade_lstm/' + date + 'test')\n",
    "\n",
    "model_checkpoint = 'lstm_self_built.chkpt'\n",
    "\n",
    "tf_saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   20 | incorrect  77.8% | mean squ error  9213.7\n",
      "Epoch   40 | incorrect  77.8% | mean squ error  8520.0\n",
      "Epoch   60 | incorrect  75.6% | mean squ error  7836.4\n",
      "Epoch   80 | incorrect  72.6% | mean squ error  7428.3\n",
      "Epoch  100 | incorrect  72.4% | mean squ error  7240.1\n",
      "Epoch  120 | incorrect  72.4% | mean squ error  7032.9\n",
      "Epoch  140 | incorrect  71.9% | mean squ error  6738.3\n",
      "Epoch  160 | incorrect  71.7% | mean squ error  6314.5\n",
      "Epoch  180 | incorrect  71.4% | mean squ error  5744.2\n",
      "Epoch  200 | incorrect  70.8% | mean squ error  5109.4\n",
      "Epoch  220 | incorrect  67.3% | mean squ error  4520.6\n",
      "Epoch  240 | incorrect  62.1% | mean squ error  3868.9\n",
      "Epoch  260 | incorrect  58.3% | mean squ error  3255.9\n",
      "Epoch  280 | incorrect  56.1% | mean squ error  2874.8\n",
      "Epoch  300 | incorrect  54.8% | mean squ error  2674.3\n",
      "Epoch  320 | incorrect  52.8% | mean squ error  2513.3\n",
      "Epoch  340 | incorrect  50.5% | mean squ error  2356.5\n",
      "Epoch  360 | incorrect  46.9% | mean squ error  2190.4\n",
      "Epoch  380 | incorrect  43.6% | mean squ error  2006.0\n",
      "Epoch  400 | incorrect  40.4% | mean squ error  1803.5\n",
      "Epoch  420 | incorrect  35.8% | mean squ error  1605.0\n",
      "Epoch  440 | incorrect  30.9% | mean squ error  1444.9\n",
      "Epoch  460 | incorrect  27.1% | mean squ error  1324.2\n",
      "Epoch  480 | incorrect  24.3% | mean squ error  1230.8\n",
      "Epoch  500 | incorrect  22.4% | mean squ error  1153.8\n",
      "Epoch  520 | incorrect  20.9% | mean squ error  1094.7\n",
      "Epoch  540 | incorrect  19.7% | mean squ error  1034.9\n",
      "Epoch  560 | incorrect  18.5% | mean squ error  986.6\n",
      "Epoch  580 | incorrect  17.5% | mean squ error  945.1\n",
      "Epoch  600 | incorrect  16.6% | mean squ error  907.0\n",
      "Epoch  620 | incorrect  15.8% | mean squ error  872.5\n",
      "Epoch  640 | incorrect  14.8% | mean squ error  843.3\n",
      "Epoch  660 | incorrect  14.3% | mean squ error  815.2\n",
      "Epoch  680 | incorrect  13.6% | mean squ error  787.9\n",
      "Epoch  700 | incorrect  13.3% | mean squ error  762.0\n",
      "Epoch  720 | incorrect  12.6% | mean squ error  737.6\n",
      "Epoch  740 | incorrect  12.0% | mean squ error  713.6\n",
      "Epoch  760 | incorrect  11.7% | mean squ error  692.2\n",
      "Epoch  780 | incorrect  11.4% | mean squ error  674.7\n",
      "Epoch  800 | incorrect  11.2% | mean squ error  655.7\n",
      "Epoch  820 | incorrect  10.9% | mean squ error  638.6\n",
      "Epoch  840 | incorrect  10.6% | mean squ error  621.9\n",
      "Epoch  860 | incorrect  10.5% | mean squ error  605.4\n",
      "Epoch  880 | incorrect  10.4% | mean squ error  588.9\n",
      "Epoch  900 | incorrect  10.3% | mean squ error  572.3\n",
      "Epoch  920 | incorrect  10.2% | mean squ error  555.4\n",
      "Epoch  940 | incorrect  9.9% | mean squ error  537.8\n",
      "Epoch  960 | incorrect  9.8% | mean squ error  522.3\n",
      "Epoch  980 | incorrect  9.4% | mean squ error  506.5\n",
      "Epoch 1000 | incorrect  9.2% | mean squ error  489.2\n",
      "Epoch 1020 | incorrect  9.1% | mean squ error  472.4\n",
      "Epoch 1040 | incorrect  9.0% | mean squ error  455.4\n",
      "Epoch 1060 | incorrect  9.0% | mean squ error  438.2\n",
      "Epoch 1080 | incorrect  8.8% | mean squ error  420.9\n",
      "Epoch 1100 | incorrect  8.5% | mean squ error  403.6\n",
      "Epoch 1120 | incorrect  8.4% | mean squ error  386.6\n",
      "Epoch 1140 | incorrect  8.1% | mean squ error  370.0\n",
      "Epoch 1160 | incorrect  7.9% | mean squ error  357.7\n",
      "Epoch 1180 | incorrect  7.6% | mean squ error  342.3\n",
      "Epoch 1200 | incorrect  7.5% | mean squ error  327.7\n",
      "Epoch 1220 | incorrect  7.2% | mean squ error  315.4\n",
      "Epoch 1240 | incorrect  7.1% | mean squ error  303.9\n",
      "Epoch 1260 | incorrect  6.9% | mean squ error  293.2\n",
      "Epoch 1280 | incorrect  6.8% | mean squ error  283.2\n",
      "Epoch 1300 | incorrect  6.5% | mean squ error  273.8\n",
      "Epoch 1320 | incorrect  6.3% | mean squ error  265.1\n",
      "Epoch 1340 | incorrect  6.3% | mean squ error  257.0\n",
      "Epoch 1360 | incorrect  6.2% | mean squ error  249.3\n",
      "Epoch 1380 | incorrect  6.2% | mean squ error  260.2\n",
      "Epoch 1400 | incorrect  6.0% | mean squ error  238.4\n",
      "Epoch 1420 | incorrect  5.9% | mean squ error  230.5\n",
      "Epoch 1440 | incorrect  5.7% | mean squ error  224.6\n",
      "Epoch 1460 | incorrect  5.5% | mean squ error  219.3\n",
      "Epoch 1480 | incorrect  5.5% | mean squ error  214.1\n",
      "Epoch 1500 | incorrect  5.4% | mean squ error  209.1\n",
      "Epoch 1520 | incorrect  5.3% | mean squ error  204.3\n",
      "Epoch 1540 | incorrect  5.2% | mean squ error  199.6\n",
      "Epoch 1560 | incorrect  5.1% | mean squ error  195.0\n",
      "Epoch 1580 | incorrect  5.0% | mean squ error  190.6\n",
      "Epoch 1600 | incorrect  4.8% | mean squ error  186.3\n",
      "Epoch 1620 | incorrect  4.9% | mean squ error  183.6\n",
      "Epoch 1640 | incorrect  4.5% | mean squ error  179.7\n",
      "Epoch 1660 | incorrect  4.6% | mean squ error  174.9\n",
      "Epoch 1680 | incorrect  4.5% | mean squ error  171.4\n",
      "Epoch 1700 | incorrect  4.4% | mean squ error  167.9\n",
      "Epoch 1720 | incorrect  4.3% | mean squ error  164.4\n",
      "Epoch 1740 | incorrect  4.2% | mean squ error  161.0\n",
      "Epoch 1760 | incorrect  4.1% | mean squ error  157.7\n",
      "Epoch 1780 | incorrect  3.9% | mean squ error  154.4\n",
      "Epoch 1800 | incorrect  3.8% | mean squ error  151.2\n",
      "Epoch 1820 | incorrect  3.7% | mean squ error  148.1\n",
      "Epoch 1840 | incorrect  3.6% | mean squ error  145.1\n",
      "Epoch 1860 | incorrect  3.5% | mean squ error  142.1\n",
      "Epoch 1880 | incorrect  3.5% | mean squ error  139.3\n",
      "Epoch 1900 | incorrect  3.5% | mean squ error  136.7\n",
      "Epoch 1920 | incorrect  3.4% | mean squ error  135.4\n",
      "Epoch 1940 | incorrect  3.4% | mean squ error  131.9\n",
      "Epoch 1960 | incorrect  3.4% | mean squ error  129.5\n",
      "Epoch 1980 | incorrect  3.3% | mean squ error  127.3\n",
      "Epoch 2000 | incorrect  3.3% | mean squ error  125.1\n",
      "Epoch 2020 | incorrect  3.3% | mean squ error  123.0\n",
      "Epoch 2040 | incorrect  3.1% | mean squ error  121.0\n",
      "Epoch 2060 | incorrect  2.9% | mean squ error  119.0\n",
      "Epoch 2080 | incorrect  2.9% | mean squ error  117.0\n",
      "Epoch 2100 | incorrect  2.8% | mean squ error  115.1\n",
      "Epoch 2120 | incorrect  2.7% | mean squ error  113.2\n",
      "Epoch 2140 | incorrect  2.7% | mean squ error  111.3\n",
      "Epoch 2160 | incorrect  2.7% | mean squ error  113.5\n",
      "Epoch 2180 | incorrect  2.6% | mean squ error  108.6\n",
      "Epoch 2200 | incorrect  2.6% | mean squ error  106.1\n",
      "Epoch 2220 | incorrect  2.5% | mean squ error  104.4\n"
     ]
    }
   ],
   "source": [
    "epoch = 4000\n",
    "\n",
    "for i in range(epoch):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        summary, incorrect, mean_squ_err = sess.run([merged, error, mean_square_error], {data: test_input, target: test_output})\n",
    "        test_writer.add_summary(summary, i)\n",
    "        \n",
    "        print('Epoch {:4d} | incorrect {: 3.1f}% | mean squ error {: 3.1f}'.format(i + 1, incorrect * 100, mean_squ_err))\n",
    "    else:\n",
    "        summary, acc = sess.run([merged, error], {data: train_input, target: train_output})\n",
    "        train_writer.add_summary(summary, i)\n",
    "    \n",
    "    sess.run(minimize,{data: train_input, target: train_output})\n",
    "    \n",
    "    if i % 100:\n",
    "        tf_saver.save(sess, model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload(trainer)\n",
    "sess.run(prediction, {data: [trainer.encodeSequence(\"II++++\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.close()\n",
    "# train_writer.close()\n",
    "# test_writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
